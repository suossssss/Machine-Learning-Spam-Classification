 so then tim peter is all like tim my test train on about number number msg and a binari pickl of the databas is approach number million byte that shrink to under number million byte though if i delet all the wordinfo record with spamprob exactli equal to unknown spamprob such record aren t need when score an unknown word get a made up probabl of unknown spamprob such record ar onli need for train i ve note befor that a score onli databas can be leaner that s pretti good i wonder how much better you could do by us some custom pickler i just check my littl dbm file and found a lot of what i would call bloat import anydbm hammi d hammi persistentgrahambay ham db db anydbm open ham db db neal len db neal ccopi regn reconstructornqxnumb cclassifiernwordinfonqxnumberc builtin nobjectnqxnumberntrqxnumb gaxcexbc xfdxnumberxbbokxnumberkxnumberkxnumberg xenumberxnumberxnumberxnumberxnumberxnumberxnumbertb number d wordinfo neal len d wordinfo neal wordinfo number number number number number number number number ignor the fact that there ar too mani zero in there the pickl version of that wordinfo object is over twice as larg as the string represent so we could get a number decreas in size just by us the string represent instead of the pickl right someth about that logic seem wrong to me but i can t see what it is mayb pickl is good for heterogen data type but everi valu of our big dictionari is go to have the same type so there s a ton of redund i guess that explain why it compress so well neal
